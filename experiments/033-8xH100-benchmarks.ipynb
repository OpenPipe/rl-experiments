{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from itertools import cycle, islice\n",
    "from lib import models\n",
    "from lib.grpo import GRPO\n",
    "from lib.pack import packed_tensors_from_tokenized_results, plot_packed_tensors\n",
    "from lib.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.tasks import ChatCompletionParams, get_task_results\n",
    "from lib.temporal_clue import get_temporal_clue_tasks\n",
    "from lib.tokenize import TaskResultTokenizer\n",
    "from lib.tune import clear_iteration_dirs, get_iteration, get_last_iteration_dir, last_tune_log, tune, Verbosity\n",
    "from lib.utils import symlink_shm, rsync_dir\n",
    "from lib.vllm import start_vllm, kill_vllm_workers\n",
    "import polars as pl\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "\n",
    "temporal_clue_tasks = list(get_temporal_clue_tasks())[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-70B --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=8 --port=8000 --api-key=default\n"
     ]
    }
   ],
   "source": [
    "vllm = await start_vllm(\n",
    "    models.distilled_llama_70b().base_model,\n",
    "    max_concurrent_requests=1024,\n",
    "    named_arguments=dict(\n",
    "        block_size=32,\n",
    "        disable_log_requests=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enforce_eager=True,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        max_model_len=16384,\n",
    "        max_num_seqs=1024,\n",
    "        max_num_batched_tokens=16384,\n",
    "        num_scheduler_steps=8,\n",
    "        preemption_mode=\"swap\",\n",
    "        return_tokens_as_token_ids=True,\n",
    "        swap_space=80,\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "    ),\n",
    "    timeout=180 + 15 * torch.cuda.device_count(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
