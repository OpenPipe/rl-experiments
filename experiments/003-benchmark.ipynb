{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(messages=[{'role': 'user', 'content': 'Find groups of four items that share something in common. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nsnow\\nlevel\\nshift\\nkayak\\nheat\\ntab\\nbucks\\nreturn\\njazz\\nhail\\noption\\nrain\\nsleet\\nracecar\\nmom\\nnets'}], grader=<function get_grader.<locals>.grader at 0x303b32f20>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Find groups of four items that share something in common. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nPUMP\\nFOOT\\nTIME\\nSEA\\nLEAGUE\\nLOAFER\\nWHY\\nUS\\nBOOT\\nYARD\\nPEOPLE\\nARE\\nMILE\\nSNEAKER\\nQUEUE\\nESSENCE'}], grader=<function get_grader.<locals>.grader at 0x303b33380>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Group words that share a common thread. There are four words for each common thread. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\namigo\\nmouth\\nlab\\nstooge\\nwolf\\nking\\nnose\\nchow\\ntenor\\npom\\nscarf\\neye\\npit\\ngobble\\npeke\\ncheek'}], grader=<function get_grader.<locals>.grader at 0x303b33560>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Group words that share a common thread. There are four words for each common thread. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nDUST\\nCATS\\nSPIDER\\nCAROUSEL\\nPUMA\\nIRON\\nNIKE\\nMOP\\nCHICAGO\\nSWEEP\\nSUPER\\nBAT\\nREEBOK\\nCABARET\\nVACUUM\\nADIDAS'}], grader=<function get_grader.<locals>.grader at 0x303b337e0>),\n",
       " Task(messages=[{'role': 'user', 'content': 'This is a puzzle. Create four groups of four. Words in each group fit under a specific category. Some categories might be defined by their use of wordplay (palindromes, homophones, adding or dropping letters and words) rather than the literal meanings of the words on the cards. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nmustard\\ntartar\\nplum\\nblue\\ngreen\\nprime\\nglum\\nrelish\\ndown\\npeacock\\nketchup\\nlow\\nhulu\\nscarlet\\nmayo\\nnetflix'}], grader=<function get_grader.<locals>.grader at 0x303abf2e0>),\n",
       " Task(messages=[{'role': 'user', 'content': 'This is a puzzle. Create four groups of four. Words in each group fit under a specific category. Some categories might be defined by their use of wordplay (palindromes, homophones, adding or dropping letters and words) rather than the literal meanings of the words on the cards. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nWONDER\\nFUTURE\\nBABY\\nGO\\nSIN\\nCHANCE\\nROYAL\\nICE CUBE\\nMIDNIGHT\\nJAIL\\nSEA\\nPOWDER\\nQ-TIP\\nSISTER\\nBOARDWALK\\nCOMMON'}], grader=<function get_grader.<locals>.grader at 0x303abdd00>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Find groups of four items that share something in common. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\njohn\\ncub\\nstar\\nsilver\\nknee\\nthrone\\njoey\\njelly\\ncalf\\nankle\\ncray\\nhead\\nshin\\ncan\\nkid\\nthigh'}], grader=<function get_grader.<locals>.grader at 0x303b336a0>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Find groups of four items that share something in common. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nLOCKSMITH\\nFIRE TRUCK\\nKING\\nPIANO\\nCHESS\\nRUBY\\nFLORIDA\\nTWIN\\nCHERRY\\nQUEEN\\nSTOP SIGN\\nGO\\nCHECKERS\\nCRYPTOGRAPHY\\nFULL\\nBACKGAMMON'}], grader=<function get_grader.<locals>.grader at 0x303abcfe0>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Group words that share a common thread. There are four words for each common thread. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nsquash\\ntee\\ncarrot\\nant\\ntank\\ncorn\\ncricket\\nmantis\\npolo\\nonion\\nbeetle\\nbeet\\ncami\\ntermite\\nhalter\\nfencing'}], grader=<function get_grader.<locals>.grader at 0x303abca40>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Group words that share a common thread. There are four words for each common thread. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nTWINS\\nTURKEY\\nGOAT\\nKIWI\\nSCALES\\nFISH\\nJORDAN\\nJAY\\nORANGE\\nGEORGIA\\nDATE\\nCRANE\\nTOGO\\nSWALLOW\\nLEMON\\nCHAD'}], grader=<function get_grader.<locals>.grader at 0x33c3afb00>),\n",
       " Task(messages=[{'role': 'user', 'content': 'This is a puzzle. Create four groups of four. Words in each group fit under a specific category. Some categories might be defined by their use of wordplay (palindromes, homophones, adding or dropping letters and words) rather than the literal meanings of the words on the cards. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nsugar\\nangel\\nginger\\nbird\\nbaby\\ncumin\\nboo\\nscary\\nsweetie\\ncoriander\\nairplane\\nclove\\npegasus\\nhoney\\ncardamom\\nposh'}], grader=<function get_grader.<locals>.grader at 0x33c3ac720>),\n",
       " Task(messages=[{'role': 'user', 'content': 'This is a puzzle. Create four groups of four. Words in each group fit under a specific category. Some categories might be defined by their use of wordplay (palindromes, homophones, adding or dropping letters and words) rather than the literal meanings of the words on the cards. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nLUST\\nPRIDE\\nTAN\\nSCHOOL\\nSLOTH\\nSEC\\nPACK\\nCOT\\nSNAIL\\nGREED\\nSIN\\nFLOCK\\nPOD\\nLORIS\\nENVY\\nTORTOISE'}], grader=<function get_grader.<locals>.grader at 0x33c3afec0>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Find groups of four items that share something in common. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nknew\\ngnome\\niota\\nfrontier\\ndelta\\nchi\\ngnocchi\\nnew\\nspirit\\nbeta\\nvirgin\\ngnaw\\nnu\\ngnat\\nunited\\ngnu'}], grader=<function get_grader.<locals>.grader at 0x33c3aef20>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Find groups of four items that share something in common. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nCANOPY\\nLINK\\nPASSPORT\\nMARX\\nPILOT\\nSONIC\\nWRIGHT\\nACCORD\\nMURPHY\\nCRASH\\nCIVIC\\nBUNK\\nMARIO\\nJONAS\\nTRUNDLE\\nWARNER'}], grader=<function get_grader.<locals>.grader at 0x33c3af740>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Group words that share a common thread. There are four words for each common thread. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nsilver\\ntweet\\nrod\\nsparrow\\nbrown\\ntackle\\nhook\\npink\\ncaw\\njones\\nviolet\\nreel\\nturquoise\\nchirp\\nlure\\ncluck'}], grader=<function get_grader.<locals>.grader at 0x33c3af240>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Group words that share a common thread. There are four words for each common thread. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nBEAN\\nESPRESSO\\nALMOND\\nFOREST\\nPECAN\\nCLEAN\\nKELLY\\nLATTE\\nPEANUT\\nEMERALD\\nAMERICANO\\nFOX\\nOLIVE\\nWALNUT\\nCAPPUCCINO\\nCASHEW'}], grader=<function get_grader.<locals>.grader at 0x33c3ae200>),\n",
       " Task(messages=[{'role': 'user', 'content': 'This is a puzzle. Create four groups of four. Words in each group fit under a specific category. Some categories might be defined by their use of wordplay (palindromes, homophones, adding or dropping letters and words) rather than the literal meanings of the words on the cards. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\ntide\\nbermuda\\nstern\\nscarf\\nlove\\nascot\\ngain\\nbow\\nright\\nera\\nport\\nbolo\\nacute\\ntie\\nall\\nstarboard'}], grader=<function get_grader.<locals>.grader at 0x33c3ace00>),\n",
       " Task(messages=[{'role': 'user', 'content': 'This is a puzzle. Create four groups of four. Words in each group fit under a specific category. Some categories might be defined by their use of wordplay (palindromes, homophones, adding or dropping letters and words) rather than the literal meanings of the words on the cards. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nGREY\\nCUP\\nBILL\\nRIBBON\\nQUAD\\nBEAR\\nHOUSE\\nLAT\\nQUINN\\nTRI\\nMEDAL\\nCOMMANDER\\nBROWN\\nTROPHY\\nPEC\\nHOWSER'}], grader=<function get_grader.<locals>.grader at 0x33c3ad8a0>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Find groups of four items that share something in common. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\ncream\\ngarlic\\npork\\nrow\\nmirror\\nbeef\\nspat\\neggshell\\npoultry\\nstake\\ntiff\\nvanilla\\nivory\\nquarrel\\nvenison\\ncrucifix'}], grader=<function get_grader.<locals>.grader at 0x33c3ac0e0>),\n",
       " Task(messages=[{'role': 'user', 'content': 'Find groups of four items that share something in common. Output them in the following format: four total lines. On each line, there should be four comma-separated items. No additional text (like group titles or descriptions) should be in the output. Also, there should not be anything in your output before or after the solution.\\nWords:\\n\\nKING\\nOAT\\nSTONE\\nRICE\\nBARLEY\\nBONG\\nDUKE\\nFORD\\nBROWN\\nSPELT\\nLEE\\nEARL\\nHOWARD\\nBARON\\nRYE\\nPRINCE'}], grader=<function get_grader.<locals>.grader at 0x33c3aefc0>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from itertools import islice\n",
    "from lib.nyt_connections import get_connections_games, get_connections_tasks\n",
    "from lib.tasks import ChatCompletionParams, get_task_results\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "games = get_connections_games()\n",
    "benchmark_tasks = list(\n",
    "    islice(get_connections_tasks(games, parse_answers_liberally=True), 20)\n",
    ")\n",
    "benchmark_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857b17d7a1cb40a79c23af79a2b7fe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepseek-r1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce396d76a454e998ad173ed2800f4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepseek-r1:high:0.6:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98d302ec2c44149a603e2e09b70bdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepseek-r1:0.6:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecdec2df33844da899c531c5fae66f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepseek-r1:1.1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cd99680c0e4650af415e5230a85c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-qwen-1.5b:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535499c2a41b44829e05ff08ffbe4918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen-2.5-7b:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39224a34be0d404a90445dc2229a71f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-qwen-14b:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0855548d55b4bbf958fdf293930572a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-qwen-14b:0.6:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1120ea2809c24517917590ffcfef47fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-qwen-32b:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13639b2d966b4da89685fc2cd931afd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-qwen-32b:0.6:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0591c8e4a4be43378fb9a942e941a31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-llama-70b:targon:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a99aa068ae46c2936660110ca38575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-llama-70b:low:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a03a919aee45299d278379940737ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-llama-70b:high:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf45b87f81449d8a9b44bf50af24d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-llama-70b:samba:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9019484e75b6412e8f48de0524b9e3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r1-llama-70b:samba:0.6:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fireworks = openai.AsyncOpenAI(\n",
    "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
    "    api_key=os.getenv(\"FIREWORKS_API_KEY\"),\n",
    ")\n",
    "openrouter = openai.AsyncOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\", api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "together = openai.AsyncOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\", api_key=os.getenv(\"TOGETHER_API_KEY\")\n",
    ")\n",
    "\n",
    "results = await asyncio.gather(\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=fireworks,\n",
    "        model=\"accounts/fireworks/models/deepseek-r1\",\n",
    "        params=ChatCompletionParams(\n",
    "            max_tokens=2**17,\n",
    "            logprobs=True,\n",
    "            top_logprobs=5,\n",
    "        ),\n",
    "        pbar_desc=\"deepseek-r1\",\n",
    "        prices=(8.0, 8.0),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=fireworks,\n",
    "        model=\"accounts/fireworks/models/deepseek-r1\",\n",
    "        params=ChatCompletionParams(\n",
    "            logit_bias={128799: -4},  # type: ignore\n",
    "            max_tokens=2**17,\n",
    "            logprobs=True,\n",
    "            temperature=0.6,\n",
    "            top_logprobs=5,\n",
    "        ),\n",
    "        pbar_desc=\"deepseek-r1:high:0.6\",\n",
    "        prices=(8.0, 8.0),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=fireworks,\n",
    "        model=\"accounts/fireworks/models/deepseek-r1\",\n",
    "        params=ChatCompletionParams(\n",
    "            max_tokens=2**17,\n",
    "            logprobs=True,\n",
    "            temperature=0.6,\n",
    "            top_logprobs=5,\n",
    "        ),\n",
    "        pbar_desc=\"deepseek-r1:0.6\",\n",
    "        prices=(8.0, 8.0),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=fireworks,\n",
    "        model=\"accounts/fireworks/models/deepseek-r1\",\n",
    "        params=ChatCompletionParams(\n",
    "            max_tokens=2**17,\n",
    "            logprobs=True,\n",
    "            temperature=0.85,\n",
    "            top_logprobs=5,\n",
    "        ),\n",
    "        pbar_desc=\"deepseek-r1:0.85\",\n",
    "        prices=(8.0, 8.0),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-qwen-1.5b\",\n",
    "        pbar_desc=\"r1-qwen-1.5b\",\n",
    "        prices=(0.18, 0.18),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"qwen/qwen-2.5-7b-instruct\",\n",
    "        pbar_desc=\"qwen-2.5-7b\",\n",
    "        params=ChatCompletionParams(\n",
    "            extra_body={\"provider\": {\"order\": [\"DeepInfra\"], \"allow_fallbacks\": False}},\n",
    "        ),\n",
    "        prices=(0.0025, 0.005),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-qwen-14b\",\n",
    "        pbar_desc=\"r1-qwen-14b\",\n",
    "        prices=(1.6, 1.6),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-qwen-14b\",\n",
    "        params=ChatCompletionParams(\n",
    "            temperature=0.6,\n",
    "        ),\n",
    "        pbar_desc=\"r1-qwen-14b:0.6\",\n",
    "        prices=(1.6, 1.6),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-qwen-32b\",\n",
    "        params=ChatCompletionParams(\n",
    "            extra_body={\"provider\": {\"order\": [\"DeepInfra\"], \"allow_fallbacks\": False}},\n",
    "        ),\n",
    "        pbar_desc=\"r1-qwen-32b\",\n",
    "        prices=(0.12, 0.18),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-qwen-32b\",\n",
    "        params=ChatCompletionParams(\n",
    "            temperature=0.6,\n",
    "            extra_body={\"provider\": {\"order\": [\"DeepInfra\"], \"allow_fallbacks\": False}},\n",
    "        ),\n",
    "        pbar_desc=\"r1-qwen-32b:0.6\",\n",
    "        prices=(0.12, 0.18),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "        params=ChatCompletionParams(\n",
    "            extra_body={\"provider\": {\"order\": [\"Targon\"], \"allow_fallbacks\": False}},\n",
    "        ),\n",
    "        pbar_desc=\"r1-llama-70b:targon\",\n",
    "        prices=(0.0, 0.0),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "        params=ChatCompletionParams(\n",
    "            logit_bias={\"</think>\": 10},\n",
    "            extra_body={\"provider\": {\"order\": [\"Targon\"], \"allow_fallbacks\": False}},\n",
    "        ),\n",
    "        pbar_desc=\"r1-llama-70b:low\",\n",
    "        prices=(0.0, 0.0),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "        params=ChatCompletionParams(\n",
    "            logit_bias={\"</think>\": -2},\n",
    "            extra_body={\"provider\": {\"order\": [\"Targon\"], \"allow_fallbacks\": False}},\n",
    "        ),\n",
    "        pbar_desc=\"r1-llama-70b:high\",\n",
    "        prices=(0.0, 0.0),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b\",\n",
    "        params=ChatCompletionParams(\n",
    "            extra_body={\"provider\": {\"order\": [\"SambaNova\"], \"allow_fallbacks\": False}},\n",
    "        ),\n",
    "        pbar_desc=\"r1-llama-70b:samba\",\n",
    "        prices=(0.7, 1.4),\n",
    "    ),\n",
    "    get_task_results(\n",
    "        tasks=benchmark_tasks,\n",
    "        client=openrouter,\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b\",\n",
    "        params=ChatCompletionParams(\n",
    "            temperature=0.6,\n",
    "            extra_body={\"provider\": {\"order\": [\"SambaNova\"], \"allow_fallbacks\": False}},\n",
    "        ),\n",
    "        pbar_desc=\"r1-llama-70b:samba:0.6\",\n",
    "        prices=(0.7, 1.4),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'object': 'error', 'type': 'invalid_request_error', 'message': \"Input should be a valid integer, unable to parse string as an integer, field: 'logit_bias.</think>.[key]', value: '</think>'\"}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m [exception \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m exception \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mexceptions][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/experiments/lib/tasks.py:85\u001b[0m, in \u001b[0;36mget_task_results.<locals>.get_task_result\u001b[0;34m(task, client, model, log_results)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chat_completion_future \u001b[38;5;129;01min\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mas_completed(\n\u001b[1;32m     69\u001b[0m     get_chat_completion(\n\u001b[1;32m     70\u001b[0m         client,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)\n\u001b[1;32m     83\u001b[0m ):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m         chat_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_completion_future\n\u001b[1;32m     86\u001b[0m         chat_completions\u001b[38;5;241m.\u001b[39mappend(chat_completion)\n\u001b[1;32m     87\u001b[0m         stats\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mchat_completion\u001b[38;5;241m.\u001b[39mid, usage\u001b[38;5;241m=\u001b[39mchat_completion\u001b[38;5;241m.\u001b[39musage)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:631\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[0;32m--> 631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/experiments/lib/chat_completions.py:98\u001b[0m, in \u001b[0;36mget_chat_completion\u001b[0;34m(client, cache, log_results, on_chunk, **create_params)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_results \u001b[38;5;129;01mor\u001b[39;00m on_chunk:\n\u001b[0;32m---> 98\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcreate_params, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m     log_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    100\u001b[0m         chat_completion_logs_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39misoformat()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.log\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_results:\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:1727\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1725\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1726\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1729\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1730\u001b[0m             {\n\u001b[1;32m   1731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1736\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1737\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1738\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1739\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1740\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1741\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1742\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1743\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1744\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1745\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1746\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1747\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1748\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1749\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1750\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1751\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1752\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1753\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1754\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1755\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1756\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1757\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1758\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1759\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1760\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1761\u001b[0m             },\n\u001b[1;32m   1762\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1763\u001b[0m         ),\n\u001b[1;32m   1764\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1765\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1766\u001b[0m         ),\n\u001b[1;32m   1767\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1768\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1769\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1770\u001b[0m     )\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/.venv/lib/python3.12/site-packages/openai/_base_client.py:1849\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1846\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1847\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1848\u001b[0m     )\n\u001b[0;32m-> 1849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/.venv/lib/python3.12/site-packages/openai/_base_client.py:1543\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1541\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1544\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1545\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1546\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1547\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1548\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1549\u001b[0m )\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/.venv/lib/python3.12/site-packages/openai/_base_client.py:1644\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1643\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1647\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1648\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1653\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'object': 'error', 'type': 'invalid_request_error', 'message': \"Input should be a valid integer, unable to parse string as an integer, field: 'logit_bias.</think>.[key]', value: '</think>'\"}}"
     ]
    }
   ],
   "source": [
    "raise [exception for result in results[1] for exception in result.exceptions][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling create with args: (<openai.resources.chat.completions.AsyncCompletions object at 0x14d273560>,) and kwargs: {'messages': [], 'model': ''}\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'id': '91278fde2f885306', 'error': {'message': 'Unable to access model . Please visit https://api.together.ai/models to view the list of supported models.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapped\n\u001b[1;32m     46\u001b[0m wrap_bound_method(together\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate, wrapper)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m together\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39m[], model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 41\u001b[0m, in \u001b[0;36mwrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:1727\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1724\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1725\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1726\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1729\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1730\u001b[0m             {\n\u001b[1;32m   1731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1736\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1737\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1738\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1739\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1740\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1741\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1742\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1743\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1744\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1745\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1746\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1747\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1748\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1749\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1750\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1751\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1752\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1753\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1754\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1755\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1756\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1757\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1758\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1759\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1760\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1761\u001b[0m             },\n\u001b[1;32m   1762\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1763\u001b[0m         ),\n\u001b[1;32m   1764\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1765\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1766\u001b[0m         ),\n\u001b[1;32m   1767\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1768\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1769\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1770\u001b[0m     )\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/.venv/lib/python3.12/site-packages/openai/_base_client.py:1849\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1846\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1847\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1848\u001b[0m     )\n\u001b[0;32m-> 1849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/.venv/lib/python3.12/site-packages/openai/_base_client.py:1543\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1541\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1544\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1545\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1546\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1547\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1548\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1549\u001b[0m )\n",
      "File \u001b[0;32m~/github/bradhilton/openpipe-rl/.venv/lib/python3.12/site-packages/openai/_base_client.py:1644\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1643\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1647\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1648\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1653\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'id': '91278fde2f885306', 'error': {'message': 'Unable to access model . Please visit https://api.together.ai/models to view the list of supported models.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}"
     ]
    }
   ],
   "source": [
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "def wrap_bound_method(method, wrapper):\n",
    "    # Get the bound instance and the original function\n",
    "    bound_instance = method.__self__\n",
    "    original_func = method.__func__\n",
    "    \n",
    "    # Create a new wrapped method\n",
    "    wrapped = wrapper(original_func)\n",
    "    \n",
    "    # Bind the wrapped function to the instance\n",
    "    import types\n",
    "    bound_method = types.MethodType(wrapped, bound_instance)\n",
    "    \n",
    "    # For the OpenAI client, we need to look at the instance's __dict__ directly\n",
    "    instance_dict = vars(bound_instance)\n",
    "    for attr_name, attr_value in instance_dict.items():\n",
    "        if getattr(attr_value, '__func__', None) is original_func:\n",
    "            setattr(bound_instance, attr_name, bound_method)\n",
    "            break\n",
    "    else:\n",
    "        # If not found in instance dict, try class dict\n",
    "        for attr_name, attr_value in bound_instance.__class__.__dict__.items():\n",
    "            if getattr(attr_value, '__func__', None) is original_func:\n",
    "                setattr(bound_instance, attr_name, bound_method)\n",
    "                break\n",
    "        else:\n",
    "            # If still not found, just monkey patch the create attribute directly\n",
    "            if hasattr(bound_instance, 'create'):\n",
    "                bound_instance.create = bound_method\n",
    "            else:\n",
    "                raise ValueError(\"Method not found and cannot patch directly\")\n",
    "    \n",
    "    return bound_method\n",
    "\n",
    "\n",
    "def wrapper(f: Callable) -> Callable:\n",
    "    async def wrapped(*args: Any, **kwargs: Any) -> Any:\n",
    "        print(f\"Calling {f.__name__} with args: {args} and kwargs: {kwargs}\")\n",
    "        return await f(*args, **kwargs)\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "wrap_bound_method(together.chat.completions.create, wrapper)\n",
    "await together.chat.completions.create(messages=[], model=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
