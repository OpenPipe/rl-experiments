{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/sky_workdir/experiments/wandb/run-20250221_140048-028</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/rl-experiments/runs/028' target=\"_blank\">028</a></strong> to <a href='https://wandb.ai/bradhilton/rl-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/rl-experiments' target=\"_blank\">https://wandb.ai/bradhilton/rl-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/rl-experiments/runs/028' target=\"_blank\">https://wandb.ai/bradhilton/rl-experiments/runs/028</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(64, 64, 872)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from itertools import cycle, islice\n",
    "from lib import models\n",
    "from lib.grpo import GRPO\n",
    "from lib.pack import packed_tensors_from_tokenized_results, plot_packed_tensors\n",
    "from lib.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.tasks import ChatCompletionParams, get_task_results\n",
    "from lib.tokenize import TaskResultTokenizer\n",
    "from lib.tune import (\n",
    "    clear_iteration_dirs,\n",
    "    get_iteration,\n",
    "    get_last_iteration_dir,\n",
    "    last_tune_log,\n",
    "    tune,\n",
    "    Verbosity,\n",
    ")\n",
    "from lib.utils import rsync_dir\n",
    "from lib.vllm import start_vllm, kill_vllm_workers\n",
    "from lib.zebra_grid import get_zebra_grid_tasks\n",
    "import polars as pl\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "\n",
    "run_name = \"028\"\n",
    "run = wandb.init(\n",
    "    project=\"rl-experiments\",\n",
    "    name=run_name,\n",
    "    id=run_name,\n",
    "    resume=\"allow\",\n",
    "    config={\"task\": \"zebra-grid\"},\n",
    ")\n",
    "\n",
    "zebra_grid_tasks = list(get_zebra_grid_tasks())\n",
    "val_tasks = zebra_grid_tasks[:64]\n",
    "test_tasks = zebra_grid_tasks[64:128]\n",
    "train_tasks = zebra_grid_tasks[128:]\n",
    "random.seed(42)\n",
    "random.shuffle(train_tasks)\n",
    "len(val_tasks), len(test_tasks), len(train_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6424c3312b44cc6aab357e4b3a13bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88bb89a0204427d97704c2f9e46ef4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a9a1a624d145b1a888e41b12a82601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GRPO params\n",
    "wandb.config[\"clip_epsilon\"] = clip_epsilon = 0.2\n",
    "wandb.config[\"entropy_coef\"] = entropy_coef = 0.0\n",
    "wandb.config[\"kl_coef\"] = kl_coef = 0.0\n",
    "wandb.config[\"tanh\"] = tanh = True\n",
    "\n",
    "expected_tokens = 300  # Expected completion tokens per task sample\n",
    "wandb.config[\"lr\"] = lr = 2e-6\n",
    "wandb.config[\"betas\"] = betas = (0.9, 0.999)  # (0.9, 0.99)\n",
    "wandb.config[\"weight_decay\"] = weight_decay = 0.01  # 0.1\n",
    "model = models.theta_8b()\n",
    "wandb.config[\"model\"] = model.base_model\n",
    "num_iterations = 1_000\n",
    "output_dir = f\"./models/{run_name}\"\n",
    "wandb.config[\"samples_per_task\"] = samples_per_task = 50\n",
    "wandb.config[\"seq_len\"] = seq_len = 16384\n",
    "wandb.config[\"stride\"] = stride = 32\n",
    "wandb.config[\"tasks_per_iter\"] = tasks_per_iter = 64\n",
    "sync_dir = output_dir  # symlink_shm(output_dir) or output_dir\n",
    "tokenizer = AutoTokenizer.from_pretrained(model.base_model)\n",
    "verbosity: Verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0003 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0003 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc83264762a481fb68b191bfe8f3fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68f4ff23fb94430891f8b1c79681cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:12<00:00,  1.84it/s, completion_tokens=464, prompt_tokens=399, reward=0.529, token_logprobs=59359]\n",
      "train: 100%|██████████| 3200/3200 [03:33<00:00,  5.57it/s, completion_tokens=467, prompt_tokens=394, reward=0.516, token_logprobs=1492880]\n",
      "Packed tensors into torch.Size([98, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "Downloading '.gitattributes' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/15a75279e8911d4c1f515986546f6fcb5ad0717c.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/15a75279e8911d4c1f515986546f6fcb5ad0717c\n",
      "Downloading 'README.md' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/65efd9e0e3baa04473b66aa4ad473e9302cf7fc6.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/65efd9e0e3baa04473b66aa4ad473e9302cf7fc6\n",
      "Downloading 'config.json' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/e02dad4243b3b0a5c9d0a08cf16ac0681958e88a.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/e02dad4243b3b0a5c9d0a08cf16ac0681958e88a\n",
      "Downloading 'generation_config.json' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/a12ff6ae937b1178ef516719319358cb836fa752.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/a12ff6ae937b1178ef516719319358cb836fa752\n",
      "Downloading 'model-00001-of-00004.safetensors' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/1897aed155ee67904ec8b5615439cc2edf1f4bd605cad5d1c9144649b544ff6d.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/1897aed155ee67904ec8b5615439cc2edf1f4bd605cad5d1c9144649b544ff6d\n",
      "Downloading 'model-00002-of-00004.safetensors' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/ff22e1dee0fb508a43c1df82fe5032340dac0bda90880726604d48f73555e450.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/ff22e1dee0fb508a43c1df82fe5032340dac0bda90880726604d48f73555e450\n",
      "Downloading 'model-00003-of-00004.safetensors' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/5d2a6e0b6eccd56521bad2243dfd4f63327682c93e57dd59b8055fda7268c85f.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/5d2a6e0b6eccd56521bad2243dfd4f63327682c93e57dd59b8055fda7268c85f\n",
      "Downloading 'model-00004-of-00004.safetensors' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/168a7e8e303fc01fec438aa7cb03805e74279869eca72a8f7b51b87d91296ce4.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/168a7e8e303fc01fec438aa7cb03805e74279869eca72a8f7b51b87d91296ce4\n",
      "Downloading 'model.safetensors.index.json' to '/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/0fd8120f1c6acddc268ebc2583058efaf699a771.incomplete'\n",
      "Download complete. Moving file to /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/blobs/0fd8120f1c6acddc268ebc2583058efaf699a771\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|47|Loss: 0.0020:  48%|████▊     | 47/98 [16:14<17:37, 20.74s/it, entropy=0.306, loss=-0.00636, policy=-0.00636]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #4 model files to ./models/028/0004\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0004 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0004 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f18ce8899643da8f3c8a046582bd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38467ac335464fb0ad60bd0114fac58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:04<00:00, 19.23it/s, completion_tokens=509, prompt_tokens=399, reward=0.56, token_logprobs=65215]\n",
      "train: 100%|██████████| 3200/3200 [03:42<00:00,  2.50s/it, completion_tokens=450, prompt_tokens=378, reward=0.561, token_logprobs=1439592]\n",
      "Deleted iteration directory ./models/028/0003\n",
      "Packed tensors into torch.Size([94, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|46|Loss: 0.0032:  49%|████▉     | 46/94 [15:49<16:30, 20.63s/it, entropy=0.219, loss=0.00324, policy=0.00324]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #5 model files to ./models/028/0005\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0005 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0005 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46022969c3134c75bee0a123774dedd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01018b2233e44f3188ec3d191f8ac34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:28<00:00,  1.52s/it, completion_tokens=522, prompt_tokens=399, reward=0.543, token_logprobs=66766]\n",
      "train: 100%|██████████| 3200/3200 [03:56<00:00,  5.26s/it, completion_tokens=473, prompt_tokens=366, reward=0.55, token_logprobs=1512069]\n",
      "Packed tensors into torch.Size([99, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|47|Loss: 0.0028:  47%|████▋     | 47/99 [16:24<18:09, 20.95s/it, entropy=0.196, loss=-0.0033, policy=-0.0033]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #6 model files to ./models/028/0006\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0006 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0006 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf02ef225904196b9ecb4c24accbf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67a870f327b4abbb4e5ca737b47e0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:05<00:00,  5.49it/s, completion_tokens=326, prompt_tokens=399, reward=0.558, token_logprobs=41727]\n",
      "train: 100%|██████████| 3200/3200 [03:34<00:00,  3.80it/s, completion_tokens=338, prompt_tokens=376, reward=0.571, token_logprobs=1082417]\n",
      "Deleted iteration directory ./models/028/0005\n",
      "Packed tensors into torch.Size([71, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|31|Loss: 0.0007:  44%|████▎     | 31/71 [12:06<15:37, 23.44s/it, entropy=0.25, loss=-0.00458, policy=-0.00458]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #7 model files to ./models/028/0007\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0007 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0007 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcdb639f68141af94e64eb59f1b42b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7fb06f622a46cb93084bbb303840ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:20<00:00,  1.19it/s, completion_tokens=498, prompt_tokens=399, reward=0.579, token_logprobs=63747]\n",
      "train: 100%|██████████| 3200/3200 [03:26<00:00, 65.02it/s, completion_tokens=473, prompt_tokens=369, reward=0.599, token_logprobs=1515030]\n",
      "Deleted iteration directory ./models/028/0006\n",
      "Packed tensors into torch.Size([99, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|43|Loss: 0.0034:  43%|████▎     | 43/99 [16:25<21:23, 22.93s/it, entropy=0.226, loss=-0.00488, policy=-0.00488]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #8 model files to ./models/028/0008\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0008 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0008 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac96d19c9a543bf8a98d146e2c13ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c504d8c31ee4a2ab9460ea722ab2a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:29<00:00,  7.65it/s, completion_tokens=614, prompt_tokens=399, reward=0.578, token_logprobs=78608]\n",
      "train: 100%|██████████| 3200/3200 [04:26<00:00, 13.81it/s, completion_tokens=541, prompt_tokens=347, reward=0.627, token_logprobs=1731087]\n",
      "Deleted iteration directory ./models/028/0004\n",
      "Packed tensors into torch.Size([112, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|52|Loss: 0.0001:  46%|████▋     | 52/112 [18:37<21:29, 21.49s/it, entropy=0.159, loss=-0.00254, policy=-0.00254]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #9 model files to ./models/028/0009\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0009 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0009 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90290e964dd646239b342de0b718d5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02853d061a7849b984b31e7213dac708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [02:44<00:00, 13.19s/it, completion_tokens=789, prompt_tokens=399, reward=0.562, token_logprobs=100950]\n",
      "train: 100%|██████████| 3200/3200 [04:38<00:00,  3.80it/s, completion_tokens=621, prompt_tokens=345, reward=0.63, token_logprobs=1986628]\n",
      "Deleted iteration directory ./models/028/0008\n",
      "Packed tensors into torch.Size([130, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|65|Loss: 0.0008:  50%|█████     | 65/130 [21:23<21:23, 19.75s/it, entropy=0.267, loss=-0.00326, policy=-0.00326]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #10 model files to ./models/028/0010\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0010 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0010 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03889545a6ed4ebaa209469763eca6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e186f1069b48219f54d382b83d2fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [03:28<00:00,  6.67s/it, completion_tokens=788, prompt_tokens=399, reward=0.576, token_logprobs=100879]\n",
      "train: 100%|██████████| 3200/3200 [05:21<00:00,  1.52s/it, completion_tokens=642, prompt_tokens=367, reward=0.603, token_logprobs=2055747]\n",
      "Deleted iteration directory ./models/028/0009\n",
      "Packed tensors into torch.Size([135, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|66|Loss: 0.0001:  49%|████▉     | 66/135 [22:01<23:01, 20.02s/it, entropy=0.292, loss=0.000109, policy=0.000109]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #11 model files to ./models/028/0011\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0011 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0011 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f48f68d3a0748488f48ef3314fdd51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e1d2cf2e784bfcaf9fa666fe1cb70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:27<00:00,  2.60s/it, completion_tokens=548, prompt_tokens=399, reward=0.588, token_logprobs=70146]\n",
      "train: 100%|██████████| 3200/3200 [04:38<00:00, 15.12it/s, completion_tokens=589, prompt_tokens=404, reward=0.551, token_logprobs=1885667]\n",
      "Deleted iteration directory ./models/028/0010\n",
      "Packed tensors into torch.Size([123, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|60|Loss: 0.0000:  49%|████▉     | 60/123 [20:10<21:10, 20.17s/it, entropy=0.236, loss=3.52, policy=3.52]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #12 model files to ./models/028/0012\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0012 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0012 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfcf3d5813d4b80b12510340e5ef152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732ba6103535409f99827e0e5d9246f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:40<00:00,  1.47s/it, completion_tokens=593, prompt_tokens=399, reward=0.593, token_logprobs=75929]\n",
      "train: 100%|██████████| 3200/3200 [04:30<00:00,  5.85s/it, completion_tokens=547, prompt_tokens=375, reward=0.594, token_logprobs=1751179]\n",
      "Deleted iteration directory ./models/028/0007\n",
      "Packed tensors into torch.Size([114, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|54|Loss: 0.0013:  47%|████▋     | 54/114 [18:55<21:02, 21.04s/it, entropy=0.142, loss=-0.00226, policy=-0.00226]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #13 model files to ./models/028/0013\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0013 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0013 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9221ebbddf3c4c8b9862fb182a2d48ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfe9f840f46400c9ae3839722b9a204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [01:16<00:00,  2.23it/s, completion_tokens=493, prompt_tokens=399, reward=0.604, token_logprobs=63084]\n",
      "train: 100%|██████████| 3200/3200 [03:26<00:00, 14.03it/s, completion_tokens=489, prompt_tokens=369, reward=0.618, token_logprobs=1563566]\n",
      "Deleted iteration directory ./models/028/0011\n",
      "Packed tensors into torch.Size([100, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|49|Loss: 0.0055:  49%|████▉     | 49/100 [16:48<17:29, 20.58s/it, entropy=0.141, loss=0.00555, policy=0.00555]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #14 model files to ./models/028/0014\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0014 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0014 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd78a98c1eb748298fed46412448444f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25a2bbf69d94c9284f789044ecbf7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [02:13<00:00,  5.54s/it, completion_tokens=607, prompt_tokens=399, reward=0.599, token_logprobs=77736]\n",
      "train: 100%|██████████| 3200/3200 [04:49<00:00,  2.23it/s, completion_tokens=589, prompt_tokens=403, reward=0.607, token_logprobs=1885839]\n",
      "Deleted iteration directory ./models/028/0012\n",
      "Packed tensors into torch.Size([123, 16384]) shape\n",
      "rsyncing /home/ubuntu/sky_workdir/experiments/models/028 to gs://atreides/openpipe/models/028\n",
      "$ tune run --nproc-per-node=1 lib.recipe.TuneRecipe --config ./models/028/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|57|Loss: 0.0080:  46%|████▋     | 57/123 [20:10<23:21, 21.23s/it, entropy=0.373, loss=-0.00377, policy=-0.00377]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration #15 model files to ./models/028/0015\n",
      "$ vllm serve /home/ubuntu/sky_workdir/experiments/models/028/0015 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=1024 --max-num-batched-tokens=16384 --num-scheduler-steps=8 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --served-model-name=./models/028/0015 --port=8000 --api-key=default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcd43c626ff4284871ffa286d82c088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d707d0ef77824ee2be366698a437f277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 128/128 [08:39<00:00,  8.26s/it, completion_tokens=1602, prompt_tokens=399, reward=0.516, token_logprobs=205012]\n"
     ]
    }
   ],
   "source": [
    "model_name = get_last_iteration_dir(output_dir) or model.base_model\n",
    "for i in range(get_iteration(output_dir), num_iterations):\n",
    "    vllm = await start_vllm(\n",
    "        model_name,\n",
    "        max_concurrent_requests=1024,\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        named_arguments=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.95,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=1024,\n",
    "            max_num_batched_tokens=16384,\n",
    "            num_scheduler_steps=8,\n",
    "            preemption_mode=\"swap\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=80,\n",
    "            tensor_parallel_size=torch.cuda.device_count(),\n",
    "        ),\n",
    "        timeout=180 + 15 * torch.cuda.device_count(),\n",
    "        verbosity=verbosity,\n",
    "    )\n",
    "    semaphore = asyncio.Semaphore(\n",
    "        int(1.33 * vllm.max_concurrent_tokens / expected_tokens)\n",
    "    )\n",
    "    offset = i * stride\n",
    "    val_results, train_results = await asyncio.gather(\n",
    "        get_task_results(\n",
    "            tasks=val_tasks,\n",
    "            client=vllm.client,\n",
    "            model=vllm.model,\n",
    "            cache=False,\n",
    "            log_results=8,\n",
    "            n=2,\n",
    "            params=ChatCompletionParams(\n",
    "                stream_options={\n",
    "                    \"include_usage\": True,\n",
    "                },\n",
    "                max_tokens=8192,\n",
    "            ),\n",
    "            pbar_desc=\"val\",\n",
    "            semaphore=semaphore,\n",
    "        ),\n",
    "        get_task_results(\n",
    "            tasks=list(islice(cycle(train_tasks), offset, offset + tasks_per_iter)),\n",
    "            client=vllm.client,\n",
    "            model=vllm.model,\n",
    "            cache=False,\n",
    "            log_results=False,\n",
    "            n=samples_per_task,\n",
    "            params=ChatCompletionParams(\n",
    "                stream_options={\n",
    "                    \"include_usage\": True,\n",
    "                },\n",
    "                max_tokens=8192,\n",
    "            ),\n",
    "            pbar_desc=\"train\",\n",
    "            semaphore=semaphore,\n",
    "            transform=TaskResultTokenizer(tokenizer),\n",
    "        ),\n",
    "    )\n",
    "    vllm.process.terminate()\n",
    "    kill_vllm_workers()\n",
    "    val_stats = val_results.stats\n",
    "    assert val_stats.grades > 0\n",
    "    assert val_stats.usages > 0\n",
    "    wandb_data = {\n",
    "        \"iteration\": i,\n",
    "        \"exceptions\": val_stats.exceptions + train_results.stats.exceptions,\n",
    "        \"reward\": val_stats.total_reward / val_stats.grades,\n",
    "        \"tokens\": round(val_stats.completion_tokens / val_stats.usages),\n",
    "    }\n",
    "    try:\n",
    "        wandb_data.update(\n",
    "            pl.DataFrame(last_tune_log(output_dir)).drop(\"step\").mean().to_dicts()[0]\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    wandb.log(wandb_data)\n",
    "    expected_tokens = wandb_data[\"tokens\"]\n",
    "    try:\n",
    "        best_iteration = (\n",
    "            wandb.Api()\n",
    "            .run(f\"{run.entity}/{run.project}/{run.id}\")\n",
    "            .history()\n",
    "            .sort_values(by=\"reward\")[\"iteration\"]\n",
    "            .iloc[-1]\n",
    "        )\n",
    "        clear_iteration_dirs(output_dir, [best_iteration, i])\n",
    "    except Exception:\n",
    "        pass\n",
    "    # see ./logs/rsync.log for output\n",
    "    asyncio.create_task(rsync_dir(sync_dir, \"gs://atreides/openpipe/\"))\n",
    "    tokenized_results = [\n",
    "        result\n",
    "        for results in train_results\n",
    "        for result in results\n",
    "        if result.advantage != 0\n",
    "    ]\n",
    "    packed_tensors = packed_tensors_from_tokenized_results(\n",
    "        tokenized_results,\n",
    "        seq_len=seq_len,\n",
    "        pad_token_id=tokenizer.pad_token_id,  # type: ignore\n",
    "    )\n",
    "    if verbosity == 2:\n",
    "        plot_packed_tensors(packed_tensors)\n",
    "    else:\n",
    "        print(f\"Packed tensors into {packed_tensors[\"tokens\"].size()} shape\")\n",
    "    optimizer_config = ComponentConfig(\n",
    "        model.tune_optimizer,\n",
    "        lr=lr,\n",
    "        betas=betas,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    if model.tune_optimizer == \"torch.optim.AdamW\":\n",
    "        optimizer_config.fused = True\n",
    "    model_name = await tune(\n",
    "        base_model=model.base_model,\n",
    "        output_dir=output_dir,\n",
    "        packed_tensors=packed_tensors,\n",
    "        model=model.tune_model,\n",
    "        model_type=model.tune_model_type,\n",
    "        config=TuneRecipeConfig(\n",
    "            optimizer=optimizer_config,\n",
    "            loss=ComponentConfig(\n",
    "                GRPO,\n",
    "                clip_epsilon=clip_epsilon,\n",
    "                entropy_coef=entropy_coef,\n",
    "                kl_coef=kl_coef,\n",
    "                tanh=tanh,\n",
    "            ),\n",
    "            shuffle=True,\n",
    "            batch_size=model.tune_max_batch_tokens // seq_len,\n",
    "            fsdp_cpu_offload=model.tune_fsdp_cpu_offload,\n",
    "            enable_activation_checkpointing=True,\n",
    "            enable_activation_offloading=True,\n",
    "            custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "            num_output_chunks=2,\n",
    "        ),\n",
    "        verbosity=verbosity,\n",
    "    )\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
